# -*- coding: utf-8 -*-
"""CinePile Starter Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jDwvPoCsg9tck3dFhVCV-h3Ny6992wCr
"""

import torch
import torch.nn as nn
import datetime
import time
import os
import json
import glob
import cv2
import numpy as np
import platform
import subprocess
from PIL import Image
from torch.nn import CrossEntropyLoss
import torch.optim as optim
from typing import Optional, List, Tuple, Any, Dict
import typing
import matplotlib.pyplot as plt
import seaborn as sns
from collections import namedtuple
from torch.nn import CrossEntropyLoss
import torch.nn.functional as F
from contextlib import nullcontext
import torch.nn.functional as F
from TOVACompression import TOVACompression, tova_compress
import math
from tqdm import tqdm
import numpy as np
from scipy import integrate
import os
import random
import subprocess
random.seed(42)

import pandas as pd
import torch
import numpy as np

from EpisodicMemory import EpisodicMemory
from MirrorNeuronEmpathyReward import (
    MirrorNeuronEmpathyReward,
    NegativeEnvironmentalImpactAvoidance,
    DopamineDrivenEmpathyReward,
    NegativeEmotionPenalty,
    FullMoralRewardCalculator,
    MoralChoiceDataset,
    MoralEmpathyTrainer,
    SelfTaskGoalReward,
    train_moral_empathy
)
# Import functions from cinepile_RL.py for RL component
from cinepile_RL import (
    normalize_string,
    evaluate_semantic_similarity,
    eval_response,
    ans_key_map,
    format_question_and_options,
    print_qa,
    get_prompt,
    fine_tune_on_cinepile,
    train_cinepile_with_rl_rewards,
)
# Import Phi-4 multimodal specific modules for video processing
from transformers import (
    AutoTokenizer,
    AutoProcessor,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)

import sys
sys.path.append('Phi4')
from Phi4.processing_phi4mm import (VideoFrame, VideoInput, VideoInputs)
from Phi4.long_video_processor import HierarchicalTokenCompression

# pip install datasets
from datasets import load_dataset
# pip install pytube #For downloading YouTube videos for finetuning.

from COCONUTWLatentThinking import (DiffusionLLMModule, Phi4COCONUTWithLatentThinking)


## load the dataset
cinepile = load_dataset("tomg-group-umd/cinepile")

"""## Load the Dataset"""

def format_question_and_options(question, options):
    """
    Formats a question and a list of options into a single string with options labeled A, B, C, etc.

    Parameters:
    - question (str): The question to be formatted.
    - options (list of str): The options for the question.

    Returns:
    - str: The formatted question and options.
    """
    formatted_string = f"{question}\n"
    option_labels = [chr(ord('A') + i) for i in range(len(options))]  # Generate option labels dynamically

    for label, option in zip(option_labels, options):
        formatted_string += f"- {label}) {option}\n"

    return formatted_string

def print_qa(rows):
    """
    Prints formatted questions and answers from a dataset.

    Parameters:
    - rows (Iterable[Dict[str, Any]]): An iterable (e.g., list or Hugging Face dataset slice)
      where each element is a dictionary with keys 'question', 'answer_key', 'answer_key_position',
      and 'choices'. 'question' is a string representing the question text; 'answer_key' is the correct
      answer text; 'answer_key_position' is the index of the correct answer in 'choices';
      'choices' is a list of strings representing the answer options.

    Returns:
    - None: This function does not return any value but prints the questions and answers to the console.
    """
    count = 1
    ans_key_to_option = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}

    for row in rows:
        question, answer_key, answer_key_position, choices = row['question'], row['answer_key'], row['answer_key_position'], row['choices']

        question_choices = format_question_and_options(question, choices)
        print(f'Question {count}:')
        print(question_choices)

        print("\nAnswer Key:")
        print(f"{ans_key_to_option[answer_key_position]}) {answer_key}")

        count += 1
        print('-'*30)

### All the questions from a particular clip
yt_clip_title = "Area 51 (2015) - Sneaking Onto the Base Scene (4/10) | Movieclips"
clip_test_dataset = cinepile['test'].filter(lambda x: x['yt_clip_title'] == yt_clip_title)

print(f'Clip: {yt_clip_title} ({clip_test_dataset[0]["yt_clip_link"]})')

print_qa(clip_test_dataset)

"""## Evaluate Model"""

import cv2
import shutil
import PIL
from PIL import Image
import pathlib
import matplotlib.pyplot as plt;

# pip install yt-dlp
# pip install scenedetect
from scenedetect import VideoManager
from scenedetect import SceneManager
from scenedetect.detectors import ContentDetector
from scenedetect.scene_manager import save_images

"""### Video Loading Utils"""

def download_video(video_url, filename, root):
    """
    Download and convert a video from a URL and save it to a specified directory.

    Parameters:
    - video_url (str): The URL of the video to be downloaded.
    - filename (str): The base name for the output file, without file extension.
    - root (str): The root directory where the 'yt_videos' folder will be created.

    Returns:
    - tuple: A tuple containing the video URL and a boolean. The boolean is True if the
      download and conversion was successful, and False otherwise.
    """

    dir_path=f"{root}/yt_videos"

    try:
        vid_prefix = os.path.join(dir_path, filename)
        full_command = [
            "yt-dlp",
            "-S",
            "height:224,ext:mp4:m4a",
            "--recode",
            "mp4",
            "-o",
            f"{vid_prefix}.mp4",
            video_url
        ]

        print(f'saving path: {vid_prefix}.mp4')

        result = subprocess.run(full_command, capture_output=True, text=True)

        if result.returncode == 0:
            print(f"Downloaded: {vid_prefix}; {video_url}")
            return video_url, True
        else:
            print(f"Failed to download or convert {video_url}. Error: {result.stderr}")
            return video_url, False

    except Exception as e:
        print(f"Exception during download or conversion of {video_url}: {e}")
        return video_url, False

def find_scenes(video_path, threshold=30.0):
    """
    Detects important scenes in a video by analyzing changes between frames and identifying significant content changes that exceed a specified threshold.

    Parameters:
    video_path (str): The file path to the video file for which scenes are to be detected.
    threshold (float): The sensitivity threshold for detecting scene changes.

    Returns:
    list of tuples: A list where each tuple contains the start and end `FrameTimecodes` of a detected scene.
    """

    # Create a video manager object for the video.
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()

    # Add ContentDetector algorithm (with a threshold).
    scene_manager.add_detector(ContentDetector(threshold=threshold))

    # Start the video manager and perform scene detection.
    video_manager.set_downscale_factor()
    video_manager.start()

    # Perform scene detection and return scene list.
    scene_manager.detect_scenes(frame_source=video_manager)

    # Each scene is a tuple of (start, end) FrameTimecodes.
    return scene_manager.get_scene_list()

def save_frames_from_scenes(video_path, scenes, output_folder):
    """
    Extracts and saves the first frame from each detected scene in a video.

    Parameters:
    - video_path (str): The file path to the video from which frames are to be extracted.
    - scenes (list): A list of scene boundaries or metadata that specifies where each scene begins and ends.
    - output_folder (str): The directory path where the extracted frames should be saved.

    Returns:
    - None: The function saves the frames to the specified directory and does not return any value.
    """
    # Ensure output directory exists.
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Initialize video manager for frame extraction.
    video_manager = VideoManager([video_path])
    video_manager.start()

    # Save the first frame of each detected scene.
    save_images(scenes, video_manager, num_images=1, output_dir=output_folder, image_name_template='$SCENE_NUMBER')

    video_manager.release()

def get_uniform_frames(video_path, num_frames=10):
    """
    This function takes a video file and returns a list of uniform frames from the video.
    :param video_path: str, path to the video file
    :param num_frames: int, number of uniform frames to return

    :return: list of frames
    """
    # check if path exists
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video file not found at {video_path}")
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    uniform_frames = np.linspace(0, total_frames-1, num_frames, dtype=int)  # picking n frames uniformly from the video
    # random_frames = random.sample(range(total_frames), num_frames)
    frames = []
    for frame_num in uniform_frames:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
    return frames

def save_frames_as_jpg(frames, frames_dir):
    # create directory if it doesn't exist
    pathlib.Path(frames_dir).mkdir(parents=True, exist_ok=True)
    for i, frame in enumerate(frames):
        cv2.imwrite(f"{frames_dir}/{i}.jpg", frame)

"""### Processing inputs, and model evaluation"""

def process_video(test_dataset, root_dir, base_folder_name='new_yt_videos', max_num_frames=10, visualize=False, ques_idx=None):
    if ques_idx is None:
        ques_idx = random.randint(0, len(test_dataset))
    data = test_dataset[ques_idx]
    clip_title, yt_link = data['yt_clip_title'], data['yt_clip_link']
    print(clip_title, yt_link)

    video_path = f"{root_dir}/{base_folder_name}/{data['movie_name']}_{yt_link.split('/')[-1]}.mp4"
    frames_dir = f"{root_dir}/{base_folder_name}_frames/_{data['movie_name']}_{yt_link.split('/')[-1]}"

    download_video(yt_link, f"{data['movie_name']}_{yt_link.split('/')[-1]}", root=root_dir)
    scenes = find_scenes(video_path)
    save_frames_from_scenes(video_path, scenes, frames_dir)

    image_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])[:-2]
    if len(image_files) < max_num_frames:
        shutil.rmtree(frames_dir)
        frames = get_uniform_frames(video_path, max_num_frames+2)[:-2]
        save_frames_as_jpg(frames, frames_dir)
        image_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])

    img_file_paths = [os.path.join(frames_dir, image_file) for image_file in image_files]
    num_frames = max_num_frames if len(img_file_paths) > max_num_frames else len(img_file_paths)
    img_file_paths = [img_file_paths[i] for i in np.linspace(0, len(img_file_paths)-1, num_frames, dtype=int)]

    if visualize:
        num_cols = 5  # Number of columns in the grid
        num_rows = (num_frames + num_cols - 1) // num_cols  # Calculate the necessary number of rows to display all images

        fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 3 * num_rows))
        axs = axs.flatten() if num_frames > 1 else [axs]

        for i, img_path in enumerate(img_file_paths):
            img = Image.open(img_path)
            axs[i].imshow(img)
            axs[i].set_title(os.path.basename(img_path), fontsize=8)
            axs[i].axis('off')

        for ax in axs[len(img_file_paths):]:  # Hide unused axes
            ax.axis('off')

        plt.tight_layout()
        plt.show()

    return data, img_file_paths

vision_and_language_dependence_prompt = '''You will be provided with subtitles from a specific scene of a movie and a few frames from that scene. After going through the movie scene and seeing the frames, please answer the question that follows. The question will have five possible answers labeled A, B, C, D, and E, please try to provide the most probable answer in your opinion. Your output should be just one of A,B,C,D,E and nothing else.

**Output Format:**
    **Answer:** <Option_key>

**Subtitles:** \n{subs}\n\nQuestion: {question}

Note: Follow the output format strictly. Only answer with the option key (A, B, C, D, E) and nothing else.'''

def get_prompt(data):
    formatted_subs = data['subtitles']
    options = data['choices']
    formatted_question = format_question_and_options(data['question'], options)

    prompt = vision_and_language_dependence_prompt.format(subs=formatted_subs, question=formatted_question)
    return prompt

"""### Gemini"""

gemini_safety_settings=[
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE",
        },
    ]
gemini_config = {'temperature': 0}

import google.generativeai as genai
GOOGLE_API_KEY = ""

genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel("gemini-pro-vision")

ROOT_DIR = 'base_dir'
data, img_file_paths = process_video(cinepile['test'], ROOT_DIR, base_folder_name='yt_videos', max_num_frames=10, visualize=True, ques_idx=1)

prompt = get_prompt(data)
print(prompt)

gemini_img_files = [PIL.Image.open(f) for f in img_file_paths]    # loading the images
gemini_img_files.insert(0, prompt)    # adding `prompt` at the begining of the list
try:
    gemini_response = gemini_model.generate_content(gemini_img_files, safety_settings=gemini_safety_settings, generation_config=gemini_config)
    gemini_response = gemini_response.text
    # print(f"Gemini: {gemini_response} || Answer Key: {mcq_data_subset[i]['answer_key_position']}")
except Exception as e:
    print(f'Failed Evaluation due to exception: {e}!')
    gemini_response = None

print(f'Gemini Response: {gemini_response}')

"""
## Response Extraction and Scoring"""

import re

def normalize_string(input_string):
    """
    Extracts and returns the option number and option text from a given string.
    The option number is expected to be a single letter followed by an optional bracket and/or period.
    The option text is any text following the option number and its bracket/period.
    If the string does not contain an option number, the entire string is considered as the option text.
    """
    input_string = input_string.replace("*", "")
    match = re.search(r"Answer:\s*([A-E])\)?\.?\s*(.*)", input_string, re.IGNORECASE)
    if match:
        option_number = match.group(1).upper()  # Normalize option number to uppercase
        option_text = match.group(2).strip()
        return option_number, option_text
    else:
        # If no option number is found after 'Answer:', consider it as no valid answer provided
        return None, input_string.strip()

def evaluate_semantic_similarity(response, answer_key_number, answer_key_text):
    """
    Evaluates whether the answer key and student response are semantically the same.
    Returns a score of 1 if they match, otherwise 0.
    """
    student_response_number, student_response_text = normalize_string(response)

    # Compare option numbers and option texts (if available) to determine a match
    if answer_key_number and student_response_number:
        if answer_key_number == student_response_number:
            if answer_key_text and student_response_text:
                # If both strings have option texts, they must match as well
                return 1 if answer_key_text.lower() == student_response_text.lower() else 0
            # If only option numbers are provided or one string lacks option text, it's a match
            return 1
    elif answer_key_text.lower() == student_response_text.lower():
        # If no option numbers are present, but the option texts match, it's also considered a match
        return 1

    return 0

def eval_response(response, answer_key_number, answer_key_text):
    return evaluate_semantic_similarity(response, answer_key_number, answer_key_text)

ans_key_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}
ans_key_opt = ans_key_map[data['answer_key_position']]
answer_key_form = f'{ans_key_opt}) '+ data['answer_key']
correct = eval_response(gemini_response, ans_key_opt, data['answer_key'])

if correct:
    print(f'Score: {correct}\n\tModel Response: "{gemini_response}" matches Answer Key: "{answer_key_form}"')
else:
    print(f'Score: {correct}\n\tModel Response: "{gemini_response}" doesn\'t match the Answer Key: "{answer_key_form}"')


def fine_tune_on_cinepile(self, dataset_path="tomg-group-umd/cinepile", output_dir="cinepile_model",
                          batch_size=4, learning_rate=5e-5, num_epochs=3, apply_freq=100,
                          use_hico=True, max_videos=100, use_hard_split=False, visual_reliance=False,
                          num_frames=16, use_rl_rewards=True):
        """
        Fine-tune the model on the CinePile dataset from Hugging Face.
        
        Args:
            model: The model to fine-tune
            dataset_path: Path or name of the CinePile dataset (default: "tomg-group-umd/cinepile" on HuggingFace)
            output_dir: Directory to save the fine-tuned self
            batch_size: Batch size for training
            learning_rate: Learning rate for training
            num_epochs: Number of training epochs
            apply_freq: Frequency (in steps) to apply continual propagation
            use_hico: Whether to use Hierarchical Token Compression for long videos
            max_videos: Maximum number of videos to process (for development/testing)
            use_hard_split: Whether to use only the hard split from the test dataset
            visual_reliance: Whether to use only questions that require visual information
            num_frames: Number of frames to extract per video
            use_rl_rewards: Whether to use RL rewards for training
            
        Returns:
            Dictionary of training metrics
        """
        import logging
        from torch.utils.data import DataLoader
        import torch.optim as optim
        from datasets import load_dataset
        import pytube
        
        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        logger = logging.getLogger(__name__)
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Create videos directory if it doesn't exist
        os.makedirs("videos", exist_ok=True)
        
        # Load the CinePile dataset from Hugging Face
        logger.info(f"Loading CinePile dataset from {dataset_path}...")
        try:
            # Load from Hugging Face datasets
            cinepile = load_dataset(dataset_path)
            
            # Use the test split for fine-tuning (it has annotations)
            dataset = cinepile["test"]
            
            # Filter the dataset if needed
            if use_hico:
                logger.info("Using Hierarchical Token Compression for long videos")
            
            # Filter for hard split if specified
            if use_hard_split:
                dataset = dataset.filter(lambda x: x['hard_split'] == "True")
                logger.info(f"Filtered to {len(dataset)} hard split examples")
            
            # Filter for visual reliance if specified
            if visual_reliance:
                dataset = dataset.filter(lambda x: x['visual_reliance'] == "True")
                logger.info(f"Filtered to {len(dataset)} visual reliance examples")
            
            # Limit the number of videos for development/testing
            if max_videos > 0:
                max_videos = min(max_videos, len(dataset))
                dataset = dataset.select(range(max_videos))
                logger.info(f"Limited to {max_videos} videos for processing")
            
            logger.info(f"Successfully loaded CinePile dataset with {len(dataset)} examples")
        except Exception as e:
            logger.error(f"Error loading dataset: {e}")
            return None
        
        # Define CinePileDataset class
        class CinePileDataset(torch.utils.data.Dataset):
            def __init__(self, dataset, processor, use_hico=False, num_frames=16):
                self.dataset = dataset
                self.processor = processor
                self.use_hico = use_hico
                self.num_frames = num_frames
                
                # Initialize HiCo if needed
                if use_hico:
                    self.hico = HierarchicalTokenCompression(max_frames=num_frames)
                
            def __len__(self):
                return len(self.dataset)
                
            def __getitem__(self, idx):
                entry = self.dataset[idx]
                
                # Extract video information
                yt_clip_link = entry["yt_clip_link"]
                question = entry["question"]
                choices = entry["choices"]
                answer_key = entry["answer_key"]
                
                # Create a unique video path
                video_path = f"videos/video_{idx:05d}.mp4"
                
                # Download the video if it doesn't exist
                if not os.path.exists(video_path):
                    try:
                        youtube = pytube.YouTube(yt_clip_link)
                        video = youtube.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()
                        video.download(filename=video_path)
                        logger.info(f"Downloaded video {idx} from YouTube")
                    except Exception as e:
                        logger.error(f"Error downloading video {idx}: {e}")
                        # Create a dummy video path
                        video_path = None
                
                # Extract frames from video
                video_frames = []
                if video_path and os.path.exists(video_path):
                    video_frames = self.extract_frames_from_video(video_path)
                
                # Format the multiple-choice question
                formatted_question = self.format_multiple_choice_question(question, choices)
                
                # Format the input text with video token
                input_text = f"<|video_0|>\n{formatted_question}\nWhat is the correct answer?"
                
                # Format the target text (the answer)
                target_text = f"The correct answer is {answer_key}."
                
                # Process video and text
                inputs = self.processor(
                    text=input_text,
                    videos=[video_frames] if video_frames else None,
                    return_tensors="pt"
                )
                
                # Tokenize target text
                target_ids = self.processor.tokenizer(
                    target_text,
                    return_tensors="pt"
                ).input_ids
                
                # Create training sample
                sample = {
                    "input_ids": inputs["input_ids"].squeeze(),
                    "attention_mask": inputs["attention_mask"].squeeze(),
                    "labels": target_ids.squeeze(),
                    "answer_key": answer_key,  # Store the correct answer for RL reward calculation
                }
                
                # Add video tensors if available
                if "input_video_embeds" in inputs:
                    sample["input_video_embeds"] = inputs["input_video_embeds"].squeeze()
                    sample["video_attention_mask"] = inputs["video_attention_mask"].squeeze()
                    sample["video_timestamps"] = inputs["video_timestamps"].squeeze()
                    sample["video_temporal_embeddings"] = inputs["video_temporal_embeddings"].squeeze()
                
                return sample
            
            def extract_frames_from_video(self, video_path):
                """Extract frames from a video file."""
                if self.use_hico:
                    # Use HiCo for adaptive temporal sampling
                    return self.hico.extract_frames(video_path)
                
                # Standard frame extraction
                import cv2
                import numpy as np
                from PIL import Image
                
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    return []
                
                # Get video properties
                fps = cap.get(cv2.CAP_PROP_FPS)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                
                # Sample frames evenly across the video
                frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
                
                video_frames = []
                for idx in frame_indices:
                    # Set the frame position
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    if not ret:
                        continue
                        
                    # Convert timestamp to seconds
                    timestamp = idx / fps
                    
                    # Convert BGR to RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    # Convert to PIL Image
                    pil_image = Image.fromarray(frame_rgb)
                    
                    # Create VideoFrame object
                    video_frames.append(VideoFrame(image=pil_image, timestamp=timestamp))
                
                cap.release()
                return video_frames
                
            def format_multiple_choice_question(self, question, choices):
                """Format a multiple-choice question with its options."""
                formatted_question = f"{question}\n\n"
                for i, choice in enumerate(choices):
                    formatted_question += f"{chr(65 + i)}. {choice}\n"
                return formatted_question
        
        # Create dataset and dataloader
        logger.info("Creating dataset and dataloader...")
        train_dataset = CinePileDataset(
            dataset,
            self.processor,
            use_hico=use_hico,
            num_frames=num_frames
        )
        
        # Define custom collate function for the DataLoader
        def collate_fn(batch):
            """Pad sequences to the same length."""
            # Find maximum lengths
            max_input_len = max(x["input_ids"].size(0) for x in batch)
            max_label_len = max(x["labels"].size(0) for x in batch)
            
            padded_batch = {}
            for key in batch[0].keys():
                if key in ["input_ids", "attention_mask"]:
                    padded_batch[key] = torch.stack([
                        torch.nn.functional.pad(
                            x[key],
                            (0, max_input_len - x[key].size(0)),
                            value=0
                        ) for x in batch
                    ])
                elif key == "labels":
                    padded_batch[key] = torch.stack([
                        torch.nn.functional.pad(
                            x[key],
                            (0, max_label_len - x[key].size(0)),
                            value=-100  # Padding token for loss calculation
                        ) for x in batch
                    ])
                elif key in ["input_video_embeds", "video_attention_mask", "video_timestamps", "video_temporal_embeddings"]:
                    # For video tensors, we need more complex padding logic
                    padded_batch[key] = torch.stack([x[key] for x in batch])
                else:
                    # For other tensors, just stack them
                    padded_batch[key] = torch.stack([x[key] for x in batch])
            
            return padded_batch
        
        # Create DataLoader with custom collate function
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=collate_fn
        )
        
        # Create optimizer and loss function
        logger.info("Setting up optimizer and loss function...")
        optimizer = optim.AdamW(self.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # Create learning rate scheduler
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))
        
        # Check if RL rewards should be used
        if use_rl_rewards:
            logger.info("Using RL rewards: +1.00 for correct answers, 0 for incorrect answers")
            
            # Custom training loop with RL rewards
            metrics = self.train_cinepile_with_rl_rewards(
                dataloader=train_dataloader,
                optimizer=optimizer,
                criterion=criterion,
                num_epochs=num_epochs,
                apply_freq=apply_freq,
                scheduler=scheduler
            )
        else:
            # Standard training without RL rewards
            logger.info("Using standard training without RL rewards")
            metrics = self.train_with_continual_propagation(
                dataloader=train_dataloader,
                optimizer=optimizer,
                criterion=criterion,
                num_epochs=num_epochs,
                apply_freq=apply_freq,
                scheduler=scheduler
            )
        
        # Save the fine-tuned model
        logger.info(f"Saving fine-tuned model to {output_dir}...")
        self.save_pretrained(output_dir)
        
        # Save training metrics
        import json
        metrics_path = os.path.join(output_dir, "training_metrics.json")
        try:
            # Convert tensor values to float for JSON serialization
            serializable_metrics = {
                "train_loss": [float(loss) for loss in metrics["train_loss"]],
                "steps": [int(step) for step in metrics["steps"]],
                "units_reinitialized": {k: int(v) for k, v in metrics["units_reinitialized"].items()}
            }
            if "eval_loss" in metrics and metrics["eval_loss"]:
                serializable_metrics["eval_loss"] = [float(loss) for loss in metrics["eval_loss"]]
            
            with open(metrics_path, "w") as f:
                json.dump(serializable_metrics, f, indent=2)
            
            logger.info(f"Training metrics saved to {metrics_path}")
        except Exception as e:
            logger.error(f"Error saving training metrics: {e}")
        
        # Play a sound to notify that fine-tuning is complete
        logger.info("Fine-tuning completed successfully.")
        play_sound("Sound/789827__josefpres__guitar-loops-113-05-verison-05-120.wav")
        
        return metrics

def train_cinepile_with_rl_rewards(self, dataloader, optimizer, criterion, num_epochs=1,
                                   apply_freq=100, scheduler=None, eval_dataloader=None):
    """
    Train the model on CinePile dataset with RL rewards.
    
    Args:
        model: The model to train
        dataloader: DataLoader for training data
        optimizer: Optimizer for parameter updates
        criterion: Loss function
        num_epochs: Number of training epochs
        apply_freq: Frequency (in steps) to apply continual propagation
        scheduler: Optional learning rate scheduler
        eval_dataloader: Optional dataloader for evaluation during training
        
    Returns:
        Dictionary of training metrics
    """
    self.train()
    device = next(self.parameters()).device
    
    metrics = {
        'train_loss': [],
        'eval_loss': [],
        'steps': [],
        'units_reinitialized': {},
        'rl_rewards': [],
        'correct_answers': 0,
        'total_answers': 0
    }
    
    # Initialize counters
    global_step = 0
    total_units_reinitialized = 0
    
    # Track reinitialization counts per layer
    for layer_id in self.unit_utilities.keys():
        metrics['units_reinitialized'][layer_id] = 0
    
    print(f"Starting training with RL rewards (+1.00 for correct, 0 for incorrect)")
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        epoch_rewards = 0.0
        num_batches = 0
        
        for batch in dataloader:
            # Move batch to device
            if isinstance(batch, torch.Tensor):
                batch = batch.to(device)
            elif isinstance(batch, dict):
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            
            # Forward pass
            optimizer.zero_grad()
            
            # Create position IDs
            position_ids = torch.arange(
                0, batch['input_ids'].shape[1], dtype=torch.long, device=device
            ).reshape(1, -1).expand(batch['input_ids'].shape[0], -1)
            
            # Forward pass
            outputs = self.forward(
                batch['input_ids'],
                batch.get('attention_mask', torch.ones_like(batch['input_ids'])),
                batch.get('labels', batch['input_ids']),
                position_ids,
                x=batch.get('input_video_embeds', None)  # Pass video embeddings if available
            )
            
            # Calculate standard loss
            base_loss = outputs.loss
            
            # Get model predictions
            logits = outputs.logits
            
            # Calculate RL rewards based on predictions
            batch_rewards = []
            
            # Process each item in the batch
            for i in range(logits.size(0)):
                # Get the predicted answer from the model's output
                # For CinePile dataset, we need to extract the letter (A, B, C, D, E) from the output
                
                # Get the predicted tokens
                pred_tokens = torch.argmax(logits[i], dim=-1)
                
                # Decode the prediction
                pred_text = self.processor.tokenizer.decode(pred_tokens)
                
                # Extract the predicted answer using functions from cinepile_RL.py
                predicted_answer_number, _ = normalize_string(pred_text)
                
                # Get the correct answer from the batch
                correct_answer = batch['answer_key'][i]
                correct_answer_position = batch['answer_key_position'][i]
                correct_answer_letter = ans_key_map.get(correct_answer_position, 'A')
                
                # Apply RL reward using eval_response from cinepile_RL.py
                # If the answer is correct, reward = 1.0, otherwise reward = 0.0
                if predicted_answer_number and predicted_answer_number == correct_answer_letter:
                    # Correct answer: +1.00 reward
                    reward = 1.0
                    metrics['correct_answers'] += 1
                else:
                    # Incorrect answer: 0 reward (neutral)
                    reward = 0.0
                
                batch_rewards.append(reward)
                metrics['total_answers'] += 1
            
            # Convert rewards to tensor
            rewards_tensor = torch.tensor(batch_rewards, device=device).float()
            
            # Apply RL rewards to the loss
            # For correct answers (reward=1.0), we want to reduce the loss
            # For incorrect answers (reward=0.0), we keep the loss as is
            
            # Method 1: Scale the loss based on rewards
            # rl_loss = base_loss * (1.0 - rewards_tensor.mean())
            
            # Method 2: Add a reward term to the loss
            # This creates a negative loss component for correct answers
            reward_term = -rewards_tensor.mean()
            rl_loss = base_loss + reward_term
            
            # Backward pass and optimization
            rl_loss.backward()
            optimizer.step()
            
            # Update metrics
            epoch_loss += base_loss.item()
            epoch_rewards += rewards_tensor.mean().item()
            num_batches += 1
            global_step += 1
            
            # Apply continual propagation at specified frequency
            if self.enable_continual_prop and global_step % apply_freq == 0:
                # Count units before reinitialization
                units_before = {layer_id: (self.unit_ages[layer_id] == 0).sum().item()
                               for layer_id in self.unit_utilities.keys()}
                
                # Apply continual propagation
                self.apply_continual_propagation()
                
                # Count reinitialized units
                for layer_id in self.unit_utilities.keys():
                    units_after = (self.unit_ages[layer_id] == 0).sum().item()
                    units_reinitialized = max(0, units_after - units_before[layer_id])
                    metrics['units_reinitialized'][layer_id] += units_reinitialized
                    total_units_reinitialized += units_reinitialized
            
            # Log progress
            if global_step % 10 == 0:
                print(f"Epoch {epoch+1}/{num_epochs}, Step {global_step}, Loss: {base_loss.item():.4f}, "
                      f"RL Reward: {rewards_tensor.mean().item():.4f}, "
                      f"Accuracy: {metrics['correct_answers']}/{metrics['total_answers']} "
                      f"({100.0 * metrics['correct_answers'] / max(1, metrics['total_answers']):.2f}%)")
        
        # Calculate average epoch loss and rewards
        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')
        avg_epoch_rewards = epoch_rewards / num_batches if num_batches > 0 else 0.0
        metrics['train_loss'].append(avg_epoch_loss)
        metrics['rl_rewards'].append(avg_epoch_rewards)
        metrics['steps'].append(global_step)
        
        # Evaluate if eval_dataloader is provided
        if eval_dataloader is not None:
            eval_loss = self.evaluate(eval_dataloader, criterion)
            metrics['eval_loss'].append(eval_loss)
            print(f"Epoch {epoch+1}/{num_epochs} completed. Train loss: {avg_epoch_loss:.4f}, "
                  f"RL Reward: {avg_epoch_rewards:.4f}, "
                  f"Eval loss: {eval_loss:.4f}")
        else:
            print(f"Epoch {epoch+1}/{num_epochs} completed. Train loss: {avg_epoch_loss:.4f}, "
                  f"RL Reward: {avg_epoch_rewards:.4f}")
        
        # Step the scheduler if provided
        if scheduler is not None:
            scheduler.step()
    
    # Print final statistics
    print(f"Training completed. Total steps: {global_step}, Total units reinitialized: {total_units_reinitialized}")
    print(f"Final accuracy: {metrics['correct_answers']}/{metrics['total_answers']} "
          f"({100.0 * metrics['correct_answers'] / max(1, metrics['total_answers']):.2f}%)")
    
    for layer_id, count in metrics['units_reinitialized'].items():
        print(f"  Layer {layer_id}: {count} units reinitialized")
    
    return metrics